{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "- Introduce a certain effect size to assess stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting c3s_511_trends.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile c3s_511_trends.py\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import iris\n",
    "import iris.pandas as ipd\n",
    "import iris.plot as iplt\n",
    "from scipy.stats.mstats import linregress\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "\n",
    "'''\n",
    "TODO BAS:\n",
    "- Check coding outlines ESMVal\n",
    "'''\n",
    "\n",
    "class TrendLims1D:\n",
    "    def __init__(self,name,datatag,verbose=True,params={'alpha' : 0.05, 'trend_magnitude' : None}):\n",
    "        self.name = name\n",
    "        self.datatag = datatag\n",
    "        self.verbose = verbose\n",
    "        self.params = params\n",
    "        # Initialize some empty data\n",
    "        self.stats_summary = {}\n",
    "        self.logbook = []\n",
    "        self.__load_data__()\n",
    "        self.fitted = {}\n",
    "    def __load_data__(self):\n",
    "        if self.datatag=='artificial':\n",
    "            raise NotImplementedError\n",
    "        elif os.path.splitext(self.datatag)[-1]=='.nc':\n",
    "            self.add_to_logbook(\"Loaded datafile {0}\".format(self.datatag))\n",
    "            datafile = os.path.join('./test_data/',self.datatag)\n",
    "            # Load the data\n",
    "            self.data_cube = iris.load_cube(datafile)\n",
    "            self.data_ts = iris.pandas.as_series(self.data_cube)\n",
    "            self.data_ts_copy = copy.deepcopy(self.data_ts)\n",
    "    def reset(self):\n",
    "        self.data_ts = self.data_ts_copy\n",
    "        self.logbook = self.logbook[1:] # Only preserve first line, with info on loading            \n",
    "    def subset(self,timeslice):\n",
    "        '''\n",
    "        Subsetting over time.\n",
    "        '''\n",
    "        self.data_ts = self.data_ts[timeslice]\n",
    "        self.add_to_logbook('Subsetted to timeperiod {0}-{1}'.format(timeslice.start,timeslice.stop))\n",
    "    \n",
    "    def decompose_seasonal(self,inplace=False,freq=None):\n",
    "        results = seasonal_decompose(self.data_ts,model='additive',freq=freq)        \n",
    "        signal = results.trend\n",
    "        seasonal = results.seasonal\n",
    "        residue = results.resid\n",
    "        # Note that the seasonal fit, is a sum of the seasonal sign and the dataset mean\n",
    "        self.fitted['seasonal'] = results.seasonal #+ self.data_ts.mean()\n",
    "        if inplace:\n",
    "            self.data_ts = self.data_ts-(self.fitted['seasonal'])\n",
    "        else:\n",
    "            return results\n",
    "    \n",
    "    def resample(self,target_freq):\n",
    "        '''\n",
    "        Resampling to the target freq\n",
    "        '''\n",
    "        self.data_ts = self.data_ts.resample(target_freq).mean()\n",
    "        self.add_to_logbook('Resampled to {0} frequency'.format(target_freq))\n",
    "    \n",
    "    def get_trends(self):\n",
    "        self.trend_mktest()\n",
    "        self.trend_linear()\n",
    "\n",
    "    def get_breakpoints(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def plot(self,label=None):\n",
    "        self.add_to_logbook(\"Creating a plot with label: {0}\".format(label))\n",
    "        self.data_ts.plot(label=label)\n",
    "        \n",
    "    def trend_mktest(self):\n",
    "        mk_input = self.data_ts.values\n",
    "        mk_input = mk_input[~np.isnan(mk_input)]\n",
    "        trend,h,p,z = self.mk_test(mk_input,alpha=self.params['alpha'])\n",
    "        # Store results in stats_summary\n",
    "        results = {}\n",
    "        results['trend']=trend\n",
    "        results['h']=h\n",
    "        results['p']=p\n",
    "        results['z']=z\n",
    "        self.stats_summary['trend_mannkendall'] = results\n",
    "        self.add_to_logbook('Calculated Mann-Kendall test')\n",
    "        \n",
    "    def trend_linear(self):\n",
    "        xaxis = self.data_ts.index.to_julian_date()\n",
    "        yaxis = self.data_ts.values\n",
    "        results = linregress(xaxis,yaxis)\n",
    "        self.fitted['trend_linear'] = xaxis*results.slope+results.intercept\n",
    "        results_dict = {}\n",
    "        results_dict['slope']=results.slope*(365.25*10) # From /day to /decade\n",
    "        if results.pvalue <= self.params['alpha']:\n",
    "            results_dict['trend'] = int(np.sign(results.slope))\n",
    "        else:\n",
    "            results_dict['trend'] = int(0)\n",
    "        results_dict['pvalue']=results.pvalue\n",
    "        results_dict['stderr']=results.stderr*(365.25*10)\n",
    "        results_dict['slope_low'] = results_dict['slope']-results_dict['stderr']\n",
    "        results_dict['slope_up'] = results_dict['slope']+results_dict['stderr']\n",
    "        self.stats_summary['trend_linear'] = results_dict\n",
    "        self.add_to_logbook('Calculated linear trend test')\n",
    "        # We are not interested in intercept and rvalue, leave them out for now\n",
    "        #results.intercept\n",
    "        #results.rvalue\n",
    "\n",
    "    # TODO\n",
    "    def trend_theilsen(self):\n",
    "        from scipy.stats.mstats import theilslopes\n",
    "\n",
    "        xaxis = self.data_ts.index.to_julian_date().values\n",
    "        yaxis = self.data_ts.values\n",
    "\n",
    "        theilsen_result = theilslopes(yaxis,x=xaxis,alpha=self.params['alpha'])\n",
    "        slope,intercept,slope_low,slope_up = theilsen_result\n",
    "        assert(slope_low <= slope <= slope_up) # Just to be safe, check this\n",
    "        slope_sign = np.sign(slope)\n",
    "        if not slope_low < 0.0 < slope_up:\n",
    "            trend = int(np.sign(slope))\n",
    "        else:\n",
    "            trend = int(0)\n",
    "        \n",
    "        results_dict = {}\n",
    "        results_dict['trend'] = trend\n",
    "        results_dict['slope'] = slope*(365.25*10) # From /day to /decade\n",
    "        results_dict['slope_low'] = slope_low*(365.25*10) # From /day to /decade\n",
    "        results_dict['slope_up'] = slope_up*(365.25*10) # From /day to /decade\n",
    "        self.stats_summary['trend_theilsen'] = results_dict\n",
    "        self.add_to_logbook('Calculated Theil-Sen slope')\n",
    "        \n",
    "        \n",
    "    def do_trends(self):\n",
    "        self.trend_mktest()\n",
    "        self.trend_linear()\n",
    "        self.trend_theilsen()\n",
    "\n",
    "    def do_residual_analysis(self,fit_name=None):\n",
    "        '''\n",
    "        # The layout of this plot follows an example by Christoph Frei in his course 'Analysis of Weather\n",
    "        # and Climate Data' at ETH Zurich\n",
    "        '''\n",
    "        res = self.data_ts.values-self.fitted[fit_name]\n",
    "\n",
    "        f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,squeeze=False,constrained_layout=True,figsize=(8,6))\n",
    "\n",
    "        # ax1 QQ plot\n",
    "        sm.qqplot(res/np.std(res),line='45',ax=ax1)\n",
    "        ax1.set_title('Normal Q-Q Plot')\n",
    "        # ax2 Histogram\n",
    "        ax2.hist(res)\n",
    "        ax2.set_title('Residual histogram')\n",
    "        ax2.set_ylabel('# of samples')\n",
    "        # testing homoscedasticity\n",
    "        ax3.scatter(self.fitted[fit_name],res)\n",
    "        ax3.set_title('Tukey-Anscombe')\n",
    "        ax3.set_ylabel('residues')\n",
    "        ax3.set_xlabel('fitted')\n",
    "\n",
    "        #TODO this needs predicted values  y = intercept + slope*x\n",
    "        nlags = 14\n",
    "\n",
    "        autocorr = acf(res,nlags=nlags)\n",
    "        lag_axis = np.array(range(nlags+1))\n",
    "        \n",
    "        # Now calculate ACF critical values\n",
    "        # From: https://stats.stackexchange.com/questions/185425/how-to-determine-the-critical-values-of-acf\n",
    "        acf_crit = np.array([lambda x: 1.96/np.sqrt(len(res)-x) if x!=0 else 1. for x in lag_axis])\n",
    "        ax4.plot(lag_axis,acf_crit)\n",
    "        ax4.plot(lag_axis,-1*acf_crit)\n",
    "        \n",
    "        ax4.bar(lag_axis,autocorr)\n",
    "        \n",
    "        ax4.set_title('Residuals ACF')\n",
    "        ax4.set_xlabel('Lag')\n",
    "        ax4.set_ylabel('ACF')\n",
    "        ax4.set_xticks(lag_axis)\n",
    "        ax4.set_xticklabels(lag_axis)\n",
    "        return f\n",
    "\n",
    "    def mk_test(self,x, alpha=0.05):\n",
    "        \"\"\"\n",
    "        This function is derived from code originally posted by Sat Kumar Tomer\n",
    "        (satkumartomer@gmail.com)\n",
    "        See also: http://vsp.pnnl.gov/help/Vsample/Design_Trend_Mann_Kendall.htm\n",
    "        The purpose of the Mann-Kendall (MK) test (Mann 1945, Kendall 1975, Gilbert\n",
    "        1987) is to statistically assess if there is a monotonic upward or downward\n",
    "        trend of the variable of interest over time. A monotonic upward (downward)\n",
    "        trend means that the variable consistently increases (decreases) through\n",
    "        time, but the trend may or may not be linear. The MK test can be used in\n",
    "        place of a parametric linear regression analysis, which can be used to test\n",
    "        if the slope of the estimated linear regression line is different from\n",
    "        zero. The regression analysis requires that the residuals from the fitted\n",
    "        regression line be normally distributed; an assumption not required by the\n",
    "        MK test, that is, the MK test is a non-parametric (distribution-free) test.\n",
    "        Hirsch, Slack and Smith (1982, page 107) indicate that the MK test is best\n",
    "        viewed as an exploratory analysis and is most appropriately used to\n",
    "        identify stations where changes are significant or of large magnitude and\n",
    "        to quantify these findings.\n",
    "        \n",
    "        #############################################################################\n",
    "        MIT License\n",
    "        Copyright (c) 2017 Michael Schramm\n",
    "        \n",
    "        Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "        of this software and associated documentation files (the \"Software\"), to deal\n",
    "        in the Software without restriction, including without limitation the rights\n",
    "        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "        copies of the Software, and to permit persons to whom the Software is\n",
    "        furnished to do so, subject to the following conditions:\n",
    "        \n",
    "        The above copyright notice and this permission notice shall be included in all\n",
    "        copies or substantial portions of the Software.\n",
    "        \n",
    "        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "        SOFTWARE.\n",
    "        #############################################################################    \n",
    "\n",
    "        \n",
    "        Input:\n",
    "            x:   a vector of data\n",
    "            alpha: significance level (0.05 default)\n",
    "        Output:\n",
    "            trend: tells the trend (increasing, decreasing or no trend)\n",
    "            h: True (if trend is present) or False (if trend is absence)\n",
    "            p: p value of the significance test\n",
    "            z: normalized test statistics\n",
    "        Examples\n",
    "        --------\n",
    "          >>> x = np.random.rand(100)\n",
    "          >>> trend,h,p,z = mk_test(x,0.05)\n",
    "        \"\"\"\n",
    "        n = len(x)\n",
    "\n",
    "        # calculate S\n",
    "        s = 0\n",
    "        for k in range(n-1):\n",
    "            for j in range(k+1, n):\n",
    "                s += np.sign(x[j] - x[k])\n",
    "\n",
    "        # calculate the unique data\n",
    "        unique_x = np.unique(x)\n",
    "        g = len(unique_x)\n",
    "\n",
    "        # calculate the var(s)\n",
    "        if n == g:  # there is no tie\n",
    "            var_s = (n*(n-1)*(2*n+5))/18\n",
    "        else:  # there are some ties in data\n",
    "            tp = np.zeros(unique_x.shape)\n",
    "            for i in range(len(unique_x)):\n",
    "                tp[i] = sum(x == unique_x[i])\n",
    "            var_s = (n*(n-1)*(2*n+5) - np.sum(tp*(tp-1)*(2*tp+5)))/18\n",
    "\n",
    "        if s > 0:\n",
    "            z = (s - 1)/np.sqrt(var_s)\n",
    "        elif s < 0:\n",
    "            z = (s + 1)/np.sqrt(var_s)\n",
    "        else: # s == 0:\n",
    "            z = 0\n",
    "\n",
    "        # calculate the p_value\n",
    "        p = 2*(1-norm.cdf(abs(z)))  # two tail test\n",
    "        h = abs(z) > norm.ppf(1-alpha/2)\n",
    "\n",
    "        if (z < 0) and h:\n",
    "            trend = -1\n",
    "        elif (z > 0) and h:\n",
    "            trend = 1\n",
    "        else:\n",
    "            trend = 0\n",
    "        return trend, h, p, z\n",
    "    def add_to_logbook(self,logmessage):\n",
    "        if self.verbose:\n",
    "            print(logmessage)\n",
    "        self.logbook.append(logmessage+'\\n')\n",
    "    def print_logbook(self):\n",
    "        print('\\n'.join(self.logbook))\n",
    "    def print_stats(self):\n",
    "        pprint.pprint(self.stats_summary)\n",
    "        \n",
    "def assess_trend_consistency(trendobject):\n",
    "    trend_consistency_score = 0\n",
    "    all_trends = [trendobject.stats_summary[trendname]['trend'] for trendname in ['trend_linear','trend_mannkendall','trend_theilsen']]\n",
    "    trend_text = {\n",
    "        -1 : 'a negative trend',\n",
    "        0  : 'no trend',\n",
    "        1 :  'a positive trend'\n",
    "    }\n",
    "    if len(set(all_trends))==1:\n",
    "        print(\"The different trend tests are fully consistent and indicate {0}.\".format(trend_text[list(set(all_trends))[0]]))\n",
    "        trend_consistency_score += 2\n",
    "    elif len(set(all_trends))==3:\n",
    "        print(\"The different trend tests are fully in consistent\")\n",
    "    elif len(set(all_trends))==2:\n",
    "        most_common = max(set(all_trends), key=all_trends.count)\n",
    "        print(\"Two out of three trend tests indicate {0}.\".format(trend_text[most_common]))\n",
    "        trend_consistency_score += 1 # Dummy statement, just for completion of the scoring\n",
    "    slope_consistency_score = 0\n",
    "    name_a,name_b='trend_linear','trend_theilsen'\n",
    "    slope_a_within_range_b = trendobject.stats_summary[name_b]['slope_low'] <= trendobject.stats_summary[name_a]['slope'] <= trendobject.stats_summary[name_b]['slope_up']\n",
    "    slope_b_within_range_a = trendobject.stats_summary[name_a]['slope_low'] <= trendobject.stats_summary[name_b]['slope'] <= trendobject.stats_summary[name_a]['slope_up']\n",
    "    overlap_of_range = max(trendobject.stats_summary[name_a]['slope_low'],trendobject.stats_summary[name_a]['slope_up']) <= min(trendobject.stats_summary[name_b]['slope_low'],trendobject.stats_summary[name_b]['slope_up'])\n",
    "    if slope_a_within_range_b and slope_b_within_range_a:\n",
    "        print(\"The two different slopes are fully consistent\")\n",
    "        slope_consistency_score += 3\n",
    "    elif slope_a_within_range_b or slope_b_within_range_a:\n",
    "        print(\"The two different slopes are partially consistent\")\n",
    "        slope_consistency_score += 2\n",
    "    else:\n",
    "        if overlap_of_range:\n",
    "            print(\"The two different slope ranges marginally overlap.\")\n",
    "            slope_consistency_score += 1\n",
    "        else:\n",
    "            print(\"The two different slopes are very inconsistent\")\n",
    "            slope_consistency_score += 0 # Dummy statement, just for completion of the scoring\n",
    "    trend_score = trend_consistency_score+slope_consistency_score\n",
    "    print(\"Final trend score: {0}\".format(trend_score))\n",
    "    return trend_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
